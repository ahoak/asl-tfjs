{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638ab14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\datittsw\\windows_projects\\asl-tfjs\\.venv\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\datittsw\\windows_projects\\asl-tfjs\\.venv\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59498c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "trainingImagesDir = '../public/assets/data/training/asl_alphabet_train'\n",
    "trainingSigns = sorted(os.listdir(trainingImagesDir))\n",
    "print(trainingSigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df4d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e51b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mproc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# cv2.ocl.setUseOpenCL(True)\n",
    "\n",
    "\n",
    "def process_sign_image(hands, sign, fileName, verbose = False):\n",
    "    tensors = []\n",
    "    \n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    file = os.path.join(trainingImagesDir, './', sign, fileName)\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    out = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if verbose:\n",
    "        plt.title(\"Processsed hands image\")\n",
    "        plt.imshow(out)\n",
    "    \n",
    "    results = hands.process(out)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            tensor = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"flattened tensor\\n\")\n",
    "                print(tensor)\n",
    "                print(\"\\n\")\n",
    "            # normalize to -1 -> 1\n",
    "            tensor = tf.linalg.normalize(tf.constant(tensor), ord=np.inf, axis=None)[0]\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"normalized tensor\\n\")\n",
    "                print(tensor)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            tensors.append(tensor)\n",
    "    return tensors\n",
    "\n",
    "def process_sign_images(sign):\n",
    "    tensors=[]\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5) as hands:\n",
    "\n",
    "        IMAGE_FILES = os.listdir(os.path.join(trainingImagesDir, './', sign))\n",
    "        for idx, fileName in enumerate(IMAGE_FILES):\n",
    "            tensors.extend(process_sign_image(hands, sign, fileName))\n",
    "\n",
    "    result = {}\n",
    "    result['sign'] = sign\n",
    "    result['tensors'] = tensors\n",
    "    return result\n",
    "\n",
    "def error_callback(r):\n",
    "    print(\"error_callback \" + str(r))\n",
    "    \n",
    "def run_parallel():\n",
    "\n",
    "    # For static images:\n",
    "    tensor_map={}\n",
    "    num_signs = len(trainingSigns)\n",
    "   \n",
    "    with mproc.Pool(mproc.cpu_count()) as p:\n",
    "        with tqdm(total=num_signs) as pbar:\n",
    "            pbar.update(0)\n",
    "            sign_idx =0 \n",
    "                \n",
    "            def process_complete(results):\n",
    "                tensors = results['tensors']\n",
    "                sign = results['sign']\n",
    "                tensor_map[sign] = tensors\n",
    "                pbar.update(1)\n",
    "            \n",
    "            results = []\n",
    "            for sign in trainingSigns:\n",
    "                r = p.apply_async(process_sign_images, [sign], callback=process_complete, error_callback=error_callback)\n",
    "                results.append(r)\n",
    "            for r in results:\n",
    "                r.wait()\n",
    "\n",
    "def run_serial(signs_to_process = None):\n",
    "\n",
    "    # For static images:\n",
    "    tensor_map={}\n",
    "    num_signs = len(trainingSigns) if signs_to_process == None else len(signs_to_process)\n",
    "\n",
    "    with tqdm(total=num_signs) as pbar:\n",
    "        pbar.update(0)\n",
    "        for sign in trainingSigns:\n",
    "            if signs_to_process == None or sign in signs_to_process:\n",
    "                print (\"Processing \" + sign)\n",
    "                tensors = process_sign_images(sign)['tensors']\n",
    "                tensor_map[sign] = tensors\n",
    "                pbar.update(1)\n",
    "\n",
    "#     for sign in trainingSigns:\n",
    "#         process_sign(sign)\n",
    "#         sign_idx = sign_idx + 1\n",
    "#         pbar.update(1)\n",
    "    return tensor_map\n",
    "\n",
    "def run_individual_image(sign, fileName, verbose = False):\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5) as hands:\n",
    "        return process_sign_image(hands, sign, fileName, verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6229a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\notebooks\\ASL Images to Landmarks.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# tensor_map = run_serial(['A'])\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000010?line=1'>2</a>\u001b[0m tensor_map \u001b[39m=\u001b[39m run_serial()\n",
      "\u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\notebooks\\ASL Images to Landmarks.ipynb Cell 4'\u001b[0m in \u001b[0;36mrun_serial\u001b[1;34m(signs_to_process)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=104'>105</a>\u001b[0m \u001b[39mif\u001b[39;00m signs_to_process \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m sign \u001b[39min\u001b[39;00m signs_to_process:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=105'>106</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m sign)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=106'>107</a>\u001b[0m     tensors \u001b[39m=\u001b[39m process_sign_images(sign)[\u001b[39m'\u001b[39m\u001b[39mtensors\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=107'>108</a>\u001b[0m     tensor_map[sign] \u001b[39m=\u001b[39m tensors\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=108'>109</a>\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\notebooks\\ASL Images to Landmarks.ipynb Cell 4'\u001b[0m in \u001b[0;36mprocess_sign_images\u001b[1;34m(sign)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=59'>60</a>\u001b[0m     IMAGE_FILES \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(trainingImagesDir, \u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m'\u001b[39m, sign))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=60'>61</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, fileName \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(IMAGE_FILES):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=61'>62</a>\u001b[0m         tensors\u001b[39m.\u001b[39mextend(process_sign_image(hands, sign, fileName))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=63'>64</a>\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=64'>65</a>\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39msign\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sign\n",
      "\u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\notebooks\\ASL Images to Landmarks.ipynb Cell 4'\u001b[0m in \u001b[0;36mprocess_sign_image\u001b[1;34m(hands, sign, fileName, verbose)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=28'>29</a>\u001b[0m     plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mProcesssed hands image\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=29'>30</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(out)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=31'>32</a>\u001b[0m results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/datittsw/windows_projects/asl-tfjs/notebooks/ASL%20Images%20to%20Landmarks.ipynb#ch0000003?line=34'>35</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hand_landmarks \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\.venv\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=131'>132</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=132'>133</a>\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=133'>134</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=134'>135</a>\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=149'>150</a>\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=150'>151</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solutions/hands.py?line=152'>153</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[1;32mc:\\Users\\datittsw\\windows_projects\\asl-tfjs\\.venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py:334\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=327'>328</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=328'>329</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=329'>330</a>\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=330'>331</a>\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=331'>332</a>\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=333'>334</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=334'>335</a>\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=335'>336</a>\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=336'>337</a>\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    <a href='file:///c%3A/Users/datittsw/windows_projects/asl-tfjs/.venv/lib/site-packages/mediapipe/python/solution_base.py?line=337'>338</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tensor_map = run_serial(['A'])\n",
    "tensor_map = run_serial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc45f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "outdir = './output/v2/'\n",
    "pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sign in tensor_map:\n",
    "    arr = np.array(tensor_map[sign])\n",
    "#     arr = np.array([i[0] for i in tensor_map[sign]])\n",
    "#     print (np.shape(tensor_map[sign].numpy()))\n",
    "    np.save(outdir + sign + '.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be00449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.10509866e-01  9.16692881e-01 -1.38889603e-06 ...  9.57289797e-01\n",
      "   7.15154060e-01 -3.13244163e-02]\n",
      " [ 8.39927646e-01  8.08220102e-01 -1.46893172e-06 ...  9.16200810e-01\n",
      "   6.73624569e-01 -6.20392429e-02]\n",
      " [ 2.57284692e-01  1.00000000e+00 -9.67722273e-07 ...  2.86345788e-01\n",
      "   8.91324910e-01 -4.80609043e-02]\n",
      " ...\n",
      " [ 8.54770636e-01  7.72019009e-01 -1.42569548e-06 ...  9.32583153e-01\n",
      "   6.12309841e-01 -6.00563080e-02]\n",
      " [ 7.84610517e-01  1.00000000e+00 -1.43675057e-06 ...  7.57687799e-01\n",
      "   4.93473930e-01 -8.43402234e-02]\n",
      " [ 2.71069461e-01  1.00000000e+00 -1.28333887e-06 ...  3.18266794e-01\n",
      "   8.75387908e-01 -3.50338332e-02]]\n"
     ]
    }
   ],
   "source": [
    "values = tensor_map['A']\n",
    "#     arr = np.array(tensor_map[sign])\n",
    "arr = np.array([i[0] for i in values])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "209b9226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n",
      "tf.Tensor([0.16666667 0.33333333 0.5        0.66666667 0.83333333 1.        ], shape=(6,), dtype=float64)\n",
      "[0.16666667 0.33333333 0.5        0.66666667 0.83333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "tensor = np.array([[res[0], res[1], res[2]] for res in [(1, 2, 3), (4, 5, 6)]]).flatten()\n",
    "\n",
    "print (tensor)\n",
    "tensor = tf.linalg.normalize(tf.constant(tensor), ord=np.inf, axis=None)[0]\n",
    "print (tensor)\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57eefad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
